{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP 논문 타임라인.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP61EAlmnZ6re7XDIiB3Ffk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/21-study-paper-review/blob/main/NLP_%EB%85%BC%EB%AC%B8_%ED%83%80%EC%9E%84%EB%9D%BC%EC%9D%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. Task-oriented Dialogue System에 대한 넓고 얕은 지식들\n",
        "\n",
        "1. A Persona-Based Neural Conversation Model\n",
        "    - 19 Mar 2016 \n",
        "\n",
        "2. Google's Neural Machine Translation \n",
        "    - 2016 년 11 월\n",
        "\n",
        "\n",
        "3. Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation\n",
        "    - 14 Nov 2016\n",
        "\n",
        "4. Adversarial Examples for Evaluating Reading Comprehension Systems\n",
        "    - 23 Jul 2017\n",
        "\n",
        "5. Unsupervised Machine Translation Using Monolingual Corpora Only\n",
        "    - 31 Oct 2017\n",
        "\n",
        "6. Graph Attention Networks(GAT)\n",
        "    - 30 Oct 2017\n",
        "\n",
        "7. Universal Sentence Encoder\n",
        "    - 29 Mar 2018\n",
        "\n",
        "8. What You Can Cram Into a Single $&!#* Vector: Probing Sentence Embeddings For Linguistc Propertie\n",
        "    - 3 May 2018\n",
        "\n",
        "9. Improving Language Understanding by Generative Pre-Training & Language Models are Unsupervised Multitask Learners\n",
        "    2018년 6월 &  Aug 2019\n",
        "\n",
        "10. Neural document summarization by jointly learning to score and select sentences\n",
        "    - 6 Jul 2018\n",
        "\n",
        "11. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
        "    - 11 Oct 2018\n",
        "\n",
        "12. Semi-Supervised Sequence Modeling With Cross-View Training\n",
        "    - 22 Sep 2018\n",
        "\n",
        "\n",
        "13. A Survey on Deep Learning for Named Entity Recognition\n",
        "    - 22 Dec 2018\n",
        "\n",
        "14. Cross-lingual Language Model Pretraining\n",
        "    - 22 Jan 2019\n",
        "\n",
        "15. RoBERTa: A Robustly Optimized BERT Pretraining Approach\n",
        "    - 26 Jul 2019\n",
        "\n",
        "16. Text Summarization with Pretrained Encoders\n",
        "    - 22 Aug 2019\n",
        "\n",
        "17. Improving Language Understanding by Generative Pre-Training & Language Models are Unsupervised Multitask Learners\n",
        "    2018년 6월 &  Aug 2019\n",
        "\n",
        "18. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\n",
        "    - 23 Mar 2020\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7jAXlqv1P7m4"
      }
    }
  ]
}