{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Classification - HuggingFace Toxic Comment.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM3PP0UnPLlVRL5fqy4PhZt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/21-study-paper-review/blob/main/BERT_Classification_HuggingFace_Toxic_Comment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT Classification - HuggingFace Toxic Comment"
      ],
      "metadata": {
        "id": "bp7pxaYeTV2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "huggingface transformers lib 사용을 위해 설치 "
      ],
      "metadata": {
        "id": "AqOjMXzuT_3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==2.3.0"
      ],
      "metadata": {
        "id": "2qcJHWE1TiJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "분석 할 데이터 압축 해제"
      ],
      "metadata": {
        "id": "wLc0vAJLUWEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_directory = 'jigsaw-toxic-comment-classification-challenge'"
      ],
      "metadata": {
        "id": "bKvsv5AST8Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "지정 경로 설정해서 압축 해제 "
      ],
      "metadata": {
        "id": "l3As6ZoNUgn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!mkdir jigsaw-toxic-comment-classification-challenge\n",
        "!unzip {dataset_directory} -d jigsaw-toxic-comment-classification-challenge"
      ],
      "metadata": {
        "id": "jmLFwAHGUjyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!unzip {dataset_directory}/train.csv.zip -d data/\n",
        "!unzip {dataset_directory}/test.csv.zip -d data/\n",
        "!unzip {dataset_directory}/test_labels.csv.zip -d data/\n",
        "!unzip {dataset_directory}/sample_submission.csv.zip -d data/"
      ],
      "metadata": {
        "id": "qbNM2fedVbAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm # 반복문에서 진행상황 확인 가능 \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "tQTKlRgJUduB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Pipeline"
      ],
      "metadata": {
        "id": "Cdh49OpHpzxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터 셋 로드 \n",
        "- 데이터 전처리 (Tokenization, Truncation & Padding)\n",
        "- 데이터 파이프라인 생성 : 데이터를 생성해서 무사히 저장하기까지 일련의 과정"
      ],
      "metadata": {
        "id": "ernZ7ZONV0i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " train_path = 'data/train.csv'\n",
        " test_path = 'data/test.csv'\n",
        " test_labels_path = 'data/test_labels.csv'\n",
        " subm_path = 'data/sample_submission.csv'\n"
      ],
      "metadata": {
        "id": "KW3j0uMVWgsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_cols = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
        "\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_test = pd.read_csv(test_path)\n",
        "df_test_labels = pd.read_csv(test_labels_path)\n",
        "df_test_labels = df_test_labels.set_index('id') # 데이터 셋의 index를 id로 변경\n",
        "df_test_labels.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XBOOTwtQWe_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\n",
        "# do_lower_case : 토큰화할 때 입력을 소문자로 할지 여부\n",
        "MAX_LEN = 128\n",
        "\n",
        "def tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n",
        "    tokenized_sentences = []\n",
        "\n",
        "    for sentence in tqdm(sentences):\n",
        "        tokenized_sentence = tokenizer.encode(\n",
        "            sentence, # Sentence to encode\n",
        "            add_special_tokens  = True , # Add '[CLS]' and '[SEP]'\n",
        "            max_length = max_seq_len, # Truencate all sentences,\n",
        "        )\n",
        "        tokenized_sentences.append(tokenized_sentence)\n",
        "    \n",
        "    return tokenized_sentences\n",
        "\n",
        "def create_attention_masks(tokenized_and_padded_sentences):\n",
        "    attention_masks = []\n",
        "\n",
        "    for sentence in tokenized_and_padded_sentences:\n",
        "        att_mask = [int(token_id > 0) for token_id in sentence]\n",
        "\n",
        "    return np.asarray(attention_masks)\n",
        "\n",
        "# 토크나이징 작업\n",
        "input_ids = tokenize_sentences(df_train['comment_text'], tokenizer, MAX_LEN)\n",
        "print(\"토크나이저 후 \" , input_ids)\n",
        "\n",
        "# 패딩 작업 \n",
        "# maxlen : 정수, 모든 시퀀스의 최대 길이\n",
        "# dtype : 출력 시퀀스의 자료형. \n",
        "# padding: 'pre' 혹은 'post': 각 시퀀스의 처음 혹은 끝을 패딩\n",
        "# truncating: 'pre' 혹은 'post': maxlen보다 큰 시퀀스의 처음 혹은 끝의 값들을 제거\n",
        "# value: 부동소수점 혹은 문자열, 패딩할 값.\n",
        "\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "print(\"패딩 후 : \", input_ids)\n",
        "\n",
        "# 패딩 된 데이터를 기반으로 어텐션 마스크 생성\n",
        "attention_masks = create_attention_masks(input_ids)\n",
        "print(\"어텐션 마스크 생성 :\", attention_masks)\n"
      ],
      "metadata": {
        "id": "iQ3ZR8GTYjQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "scikit-learn의 패키지 안에 train_test_split 모듈을 활용하여 train set(학습 데이터 셋)과 validation set(테스트 셋)을 분리"
      ],
      "metadata": {
        "id": "ECszK79SppVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# scikit-learn의 패키지 안에 train_test_split 모듈을 활용하여 train set(학습 데이터 셋)과 validation set(테스트 셋)을 분리\n",
        "# 기존 train / test로 구분 되어 있었던 데이터 셋을 \n",
        "# train에서 train / validation으로 일정 비율 쪼갠 다음에 학습 시에는 train 셋으로 학습 후 중간중간 validation 셋으로 학습한 모델 평가\n",
        "# 매 epoch 마다 validation의 오차율을 확인하면서 과적합을 방지해야 좋은 성능의 모델을 만들 수 있다.\n",
        "\n",
        "labels = df_train[label_cols].values\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=0, test_size=0.1)\n",
        "\n",
        "train_size = len(train_inputs)\n",
        "validation_size = len(validation_inputs)\n"
      ],
      "metadata": {
        "id": "g9l_sy0HeHva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32 \n",
        "NR_EPOCHES = 1\n",
        "\n",
        "def create_dataset(data_tuple, epochs=1, batch_size=32, buffer_size=10000, train=True):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data_tuple) # from_tensor_slices : 차원을 맞춰서 데이터를 변환 ex) .slices(([1,2,],[3,4,],[5,6])) -> ((1,3,5),(2,4,6))\n",
        "\n",
        "    if train: \n",
        "        dataset = dataset.shuffle(buffer_size=buffer_size) # shuffle : 버퍼에서 요소를 무작위로 샘플링하여 선택한 요소를 새 요소로 바꾼다.\n",
        "    \n",
        "    dataset = dataset.repeat(epochs) # repeat : 데이터 세트를 반복하여 생성 ex)  [1, 2, 3].repeat(3) -> [1,2,3,1,2,3,1,2,3,1,2,3]\n",
        "    dataset = dataset.batch(batch_size) # batch : 데이터 세트의 요소를 일괄 처리로 결합 ex) (1,2,3,4,5,6,7,8,9).batch_size(3) -> (1,2,3),(4,5,6),(7,8,9)\n",
        "\n",
        "    if train:\n",
        "        dataset = dataset.prefetch(1) # prefetch : Dataset이 데이터세트에서 요소를 미리 가져 오는 것 , \n",
        "\n",
        "    return dataset \n",
        "\n",
        "\n",
        "train_dataset = create_dataset((train_inputs, train_labels), epochs=NR_EPOCHES, batch_size=BATCH_SIZE)\n",
        "validation_dataset = create_dataset((validation_inputs, validation_labels),epochs=NR_EPOCHES, batch_size=BATCH_SIZE)\n",
        "\n",
        "#mask 사라짐??? 코드 오류? \n",
        "# train_dataset = create_dataset((train_inputs, train_masks, train_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "Smc43DuKpssr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT Model "
      ],
      "metadata": {
        "id": "JyMAkPtKvHLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Transformers library에서  pretrained BERT base-model을 로드 \n",
        "- BERT 출력(CLS 토큰에 해당)에서 첫 번째 hidden-state 를 취하여 6개의 뉴런과 시그모이드 활성화(Classifier)가 있는 Dense layer에 공급한다. \n",
        "- 이 계층의 출력은 6개의 클래스 각각에 대한 확률로 해석될 수 있다.\n"
      ],
      "metadata": {
        "id": "JbGu5sz-wj3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertModel\n",
        "from tensorflow.keras.layers import Dense, Flatten \n",
        "\n",
        "class BertClassifier(tf.keras.Model):\n",
        "    def __init__(self, bert: TFBertModel, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.bert = bert \n",
        "        self.classifier = Dense(num_classes, activation='sigmoid')\n",
        "\n",
        "    @tf.function \n",
        "    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids,\n",
        "                            attention_mask = attention_mask,\n",
        "                            token_type_ids=token_type_ids,\n",
        "                            position_ids= position_ids,\n",
        "                            head_mask=head_mask)\n",
        "        cls_output = outputs[1]\n",
        "        cls_output = self.classifier(cls_output)\n",
        "\n",
        "        return cls_output\n",
        "model = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(label_cols))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2ZFkNTS2xNWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training Loop"
      ],
      "metadata": {
        "id": "3VLX8JR2yh6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BinaryCrossentropy를 손실 함수로 사용합니다(출력 6개의 각 출력 뉴런에 대해 계산됨...).이는 6개의 이진 분류 작업을 동시에 교육하는 것과 같습니다.)\n",
        "\n",
        "- Transformers 라이브러리에서 AdamW optimizer를 1-cycle-policy과 함께 사용\n",
        "\n",
        "- AUC 평가 지표"
      ],
      "metadata": {
        "id": "LSGGrV9myt3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time \n",
        "from transformers import create_optimizer\n",
        "\n",
        "steps_per_epoch = train_size // BATCH_SIZE\n",
        "validation_steps = validation_size // BATCH_SIZE\n",
        "\n",
        "# | Loss Function \n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss') # metrics.Mean : 주어진 값의 (가중된) 평균을 계산, ex) [1, 3, 5, 7]이면 평균은 4. 가중치가 [1, 1, 0, 0]으로 지정된 경우 평균은 2\n",
        "validation_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "\n",
        "# | Optimizer (with 1-cycle-policy)\n",
        "warmup_steps = steps_per_epoch // 3 \n",
        "total_steps = steps_per_epoch * NR_EPOCHS - warmup_steps \n",
        "optimizer = create_optimizer(init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)\n",
        "\n",
        "# | Metrics\n",
        "train_auc_metrics = [tf.keras.metrixs.AUC() for i in range(len(label_cols))] # metrixs.AUC() :  리만 합계를 통해 근사 AUC (곡선 아래 영역)를 계산\n",
        "validation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, token_ids, masks, labels):\n",
        "    labels = tf.dtypes.cast(labels, tf.float32)\n",
        "\n",
        "    predictions = model(token_ids, attention_mask=masks, training=False)\n",
        "    v_loss = loss_object(labels, predictions)\n",
        "\n",
        "    validation_loss(v_loss)\n",
        "    for i, auc in enumerate(validation_auc_metrics):\n",
        "        auc.update_state(labels[:,i], predictions[:,i])\n",
        "\n",
        "def train(model, train_dataset, val_dataset, train_steps_per_epoch, val_steps_per_epoch, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        print('=' * 50, f\"EPOCH{epoch}\",'='*50)\n",
        "        start = time.time()\n",
        "\n",
        "        for i, (token_ids, masks, labels) in enumerate(tqdm(trainn_dataset, total=train_steps_per_epoch)):\n",
        "            train_step(model, token_ids, masks, labels)\n",
        "            if i % 1000 == 0:\n",
        "                print(f'\\nTrain Step: {i}, Loss: {train_loss.result()}')\n",
        "                for i, label_name in enumertate(label_cols):\n",
        "                    print(f\"{label_name} roc_auc {train_auc_metrics[i].result()}\")\n",
        "                    train_auc_metrics[i].reset_states()\n",
        "        \n",
        "        for i, (token_ids, masks, labels) in enumerate(tqdm(val_dataset, total=val_steps_per_epoch)):\n",
        "            validation_step(model, token_ids, masks, labels)\n",
        "        \n",
        "        print(f'\\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\n",
        "\n",
        "        for i, label_name in enumerate(label_cols):\n",
        "            print(f\"{label_name} roc_auc {validation_auc_metrics[i].result()}\")\n",
        "            validation_auc_metrics[i].reset_states()\n",
        "\n",
        "        print('\\n')\n",
        "\n",
        "train(model, train_dataset, validation_dataset, train_steps_per_epoch=steps_per_epoch, val_steps_per_epoch=validation_steps, epochs=NR_EPOCHES)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j4ctoSBhzIZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "evhPrQ8Bw73V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}