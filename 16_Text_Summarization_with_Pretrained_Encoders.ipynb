{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "16 Text Summarization with Pretrained Encoders.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMKSA0EuVXxGq85D3ogJENy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/study-paper-review/blob/main/16_Text_Summarization_with_Pretrained_Encoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarization with Pretrained Encoders"
      ],
      "metadata": {
        "id": "hz4ul1jmz5Lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "트랜스포머의 양방향 인코더 표현(BERT; Devlin 등 2019)은 최근 광범위한 자연어 처리 작업을 발전시킨 사전 훈련된 언어 모델의 최신 화신을 나타낸다. \n",
        "\n",
        "본 논문에서, 우리는 BERT를 텍스트 요약에 유용하게 적용할 수 있는 방법을 소개하고 추출 모델과 추상 모델 모두에 대한 일반적인 프레임워크를 제안한다. \n",
        "\n",
        "문서의 의미를 표현하고 문장에 대한 표현을 얻을 수 있는 BERT를 기반으로 하는 새로운 문서 수준 인코더를 소개한다. \n",
        "\n",
        "우리의 추출 모델은 여러 문장 간 트랜스포머 레이어를 쌓아서 이 인코더 위에 구축된다. \n",
        "\n",
        "추상적 요약을 위해, 우리는 둘 사이의 불일치를 완화하기 위한 수단으로 인코더와 디코더를 위해 서로 다른 최적화 프로그램을 채택하는 새로운 미세 조정 일정을 제안한다(전자는 사전 교육되지만 후자는 그렇지 않음). \n",
        "\n",
        "우리는 또한 2단계 미세 조정 접근법이 생성된 요약의 품질을 더욱 향상시킬 수 있음을 보여준다.\n",
        "\n",
        "세 가지 데이터 세트에 대한 실험은 우리 모델이 추출 및 추상 설정 모두에서 전반적으로 최첨단 결과를 달성한다는 것을 보여준다."
      ],
      "metadata": {
        "id": "6ediSOx1z7D3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "언어 모델 사전 훈련은 감정 분석에서 질문 답변, 자연어 추론, 명명된 엔티티 인식 및 텍스트 유사성에 이르는 많은 NLP 작업에서 최첨단 기술을 발전시켰다. \n",
        "\n",
        "최첨단 사전 교육 모델에는 ELMo(Peters 등, 2018), GPT(Radford 등, 2018) 및 보다 최근의 트랜스포머 양방향 인코더 표현(BERT; Devlin 등 2019)이 포함된다. \n",
        "\n",
        "BERT는 단어와 문장 표현을 하나의 매우 큰 트랜스포머(Vaswani 등, 2017)에 결합한다. \n",
        "\n",
        "그것은 마스크된 언어 모델링 및 다음 문장 예측이라는 감독되지 않은 목표를 가지고 방대한 양의 텍스트에 사전 훈련되며 다양한 작업별 목표로 미세 조정할 수 있다. \n",
        "\n",
        "대부분의 경우, 사전 훈련된 언어 모델은 다양한 분류 과제(예: 두 문장이 수반 관계에 있는지의 여부 예측)와 관련된 문장 및 문단 수준의 자연어 이해 문제의 인코더로 채택되었다(Devlin 등, 2019).나는 문장이 있다. \n",
        "\n",
        "본 논문에서, 우리는 언어 모델이 텍스트 요약에 미치는 영향을 조사한다. \n",
        "\n",
        "이전 과제와 달리 요약에는 개별 단어와 문장의 의미를 넘어 광범위한 자연어 이해가 필요하다. \n",
        "\n",
        "그 목적은 문서의 의미를 대부분 보존하면서 문서를 더 짧은 버전으로 압축하는 것이다. \n",
        "\n",
        "더욱이 추상적 모델링 공식에서, 추출 요약은 종종 텍스트 스팬(일반적으로 문장)을 나타내는 라벨이 있는 이진 분류 작업으로 정의되는 반면, 추출 요약은 원본 텍스트에 포함되지 않은 새로운 단어와 구문을 포함하는 요약을 만들기 위해 언어 생성 기능이 필요하다. 요약에 포함되어야 한다. \n",
        "\n",
        "우리는 추출 및 추상 모델링 패러다임을 모두 포함하는 일반적인 프레임워크에서 텍스트 요약에 대한 BERT의 잠재력을 탐구한다. \n",
        "\n",
        "우리는 문서를 인코딩하고 문장의 표현을 얻을 수 있는 BERT 기반의 새로운 문서 수준 인코더를 제안한다. \n",
        "\n",
        "우리의 추출 모델은 문장 추출을 위한 문서 수준 기능을 캡처하기 위해 여러 문장 간 트랜스포머 레이어를 쌓음으로써 이 인코더 위에 구축된다. \n",
        "\n",
        "우리의 추상적 모델은 동일한 사전 훈련된 BERT 인코더를 방대한 양의 텍스트에 제약된 무작위로 초기화된 트랜스포머와 마스킹 언어 모델링 및 다음 문장 예측이라는 감독되지 않은 목표와 결합하고 다양한 작업별 목표로 미세 조정할 수 있다. \n",
        "\n",
        "대부분의 경우, 사전 훈련된 언어 모델은 다양한 분류 과제(예: 두 문장이 수반 관계에 있는지의 여부 예측)와 관련된 문장 및 문단 수준의 자연어 이해 문제의 인코더로 채택되었다(Devlin 등, 2019).나는 문장이 있다. \n",
        "\n",
        "본 논문에서, 우리는 언어 모델이 텍스트 요약에 미치는 영향을 조사한다. 이전 과제와 달리 요약에는 개별 단어와 문장의 의미를 넘어 광범위한 자연어 이해가 필요하다. \n",
        "\n",
        "그 목적은 문서의 의미를 대부분 보존하면서 문서를 더 짧은 버전으로 압축하는 것이다. \n",
        "\n",
        "더욱이 추상적 모델링 공식에서, 추출 요약은 종종 텍스트 스팬(일반적으로 문장)을 나타내는 라벨이 있는 이진 분류 작업으로 정의되는 반면, 추출 요약은 원본 텍스트에 포함되지 않은 새로운 단어와 구문을 포함하는 요약을 만들기 위해 언어 생성 기능이 필요하다. \n",
        "\n",
        "요약에 포함되어야 한다. 우리는 추출 및 추상 모델링 패러다임을 모두 포함하는 일반적인 프레임워크에서 텍스트 요약에 대한 BERT의 잠재력을 탐구한다. 우리는 문서를 인코딩하고 문장의 표현을 얻을 수 있는 BERT 기반의 새로운 문서 수준 인코더를 제안한다. \n",
        "\n",
        "우리의 추출 모델은 문장 추출을 위한 문서 수준 기능을 캡처하기 위해 여러 문장 간 트랜스포머 레이어를 쌓음으로써 이 인코더 위에 구축된다. 우리의 추상적 모델은 동일한 사전 훈련된 BERT 인코더를 무작위로 초기화된 Transformer de와 결합하는 인코더-디코더 아키텍처를 채택한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "2rM9mr4cgElt"
      }
    }
  ]
}