{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Pretraining 방법 .ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM4ZHZ25ncyQ5kpQLHj+Lor",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/21-study-paper-review/blob/main/BERT_Pretraining_%EB%B0%A9%EB%B2%95_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Instance로 Pretraining 시키는 과정"
      ],
      "metadata": {
        "id": "Sf-GE5WjTCI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Training Instance를 만드는 작업이 마무리되면 "
      ],
      "metadata": {
        "id": "g5pWorKpTaqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "<입력 Sequence - 인코딩 됨>\n",
        "\n",
        " 2      4    523  8 312   1   53 5234   4   323  123   3    21  33  22   21  123   3\n",
        "\n",
        "<입력 Mask> (Padding을 마스킹하기 위해서)\n",
        "\n",
        "1 ... 1 0 ... 0 (뒤에 0들은 padding 된 index들)\n",
        "\n",
        "\n",
        "<마스킹 된 토큰의 라벨 - 인코딩 됨>\n",
        "\n",
        "3444, 553\n",
        "\n",
        "\n",
        "<마스킹 된 토큰의 위치>\n",
        "\n",
        "1, 8\n",
        "\n",
        "\n",
        "<Random Nex>\n",
        "\n",
        "1 (True)\n",
        "\n",
        "\n",
        "<Sequence ID> (해당 토큰이 Seq_A에 속하는지 Seq_B에 속하는지)\n",
        "\n",
        "0 ... 0 1 ... 1\n",
        " \n",
        "\n",
        "이제 Training Instance를 사용해서 BERT를 Pretraining 시킬 차례"
      ],
      "metadata": {
        "id": "6UzOl-qZTlie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding"
      ],
      "metadata": {
        "id": "AxCjvxe9Tk1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT를 Pretraining 시키기 전에 Encoding 한 토큰을 Embedding 해야 함."
      ],
      "metadata": {
        "id": "8wVPlNWfT7ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Encoding을 통해 토큰을 구분되는 Index로 변화시킴\n",
        "\n",
        "- 하지만 각 단어가 고유한 Index를 가지기 때문에 (One-hot Vector과 비슷하게), 단어 사이에 의미의 유사성이 없음.\n",
        "\n",
        "- 단어를 Encoding만 하게 되면 모델로써는 두 단어가 어떤 관계에 있는지 알 수 없음.\n",
        "\n",
        "- 이 문제를 해결하기 위해 나온 방법이 Embedding이다. \n",
        "\n",
        "- 이외에도 Embedding을 하는 이유는 굉장히 많다.\n",
        "\n"
      ],
      "metadata": {
        "id": "lMJl4dshU2UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ySZjjvXnVsRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zA0-d4B_Vpcw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}