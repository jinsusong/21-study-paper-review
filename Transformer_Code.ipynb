{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer Code.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNuL4ad5nilZRjnIp7s0cQe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/21-study-paper-review/blob/main/Transformer_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer : Attention is All You Need"
      ],
      "metadata": {
        "id": "84pSvpZ_7DZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2021년 기준으로 최신 고성능 모델들은 Transformer 아키텍처를 기반으로 함 \n",
        "\n",
        "GPT : Transformer의 디코더 아키텍처를 활용\n",
        "BERT : Transformer의 인코더 아키텍처를 활용\n",
        "\n"
      ],
      "metadata": {
        "id": "_6sT2chJ7Ict"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "마운트 구글 드라이브"
      ],
      "metadata": {
        "id": "EV3iB6gOB_DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%pwd\n",
        "\n",
        "\"\"\" \n",
        "Use this javascript code in inspect>console so you wont need to click the page every 15 min:\n",
        "\n",
        "########################\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "########################\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0_3MDywzB0tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "change current path to where the working project folder is at "
      ],
      "metadata": {
        "id": "9ZJ_yddaCgwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/projects/transformers_translation/"
      ],
      "metadata": {
        "id": "7q2JK-gHCKG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0 : Get The Data "
      ],
      "metadata": {
        "id": "BNXzSWPXCrNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "upload the data to our current path and unzip it (numcomment and run this only once)"
      ],
      "metadata": {
        "id": "a9SU629ICwd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # data is from: https://www.statmt.org/europarl/ you can use this or just upload your own data\n",
        "# %cd data\n",
        "# !wget https://www.statmt.org/europarl/v7/de-en.tgz\n",
        "# !tar -xvf de-en.tgz\n",
        "# %cd ..\n",
        "# %pwd"
      ],
      "metadata": {
        "id": "Ab7v2fU2DAuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get non breaking prefixs"
      ],
      "metadata": {
        "id": "6-xucs7EDDW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get non_breaking_prefixes from https://github.com/moses-smt/mosesdecoder/tree/master/scripts/share/nonbreaking_prefixes\n",
        "# then rename them to: \"nonbreaking_prefix.en\" and \"nonbreaking_prefix.de\" and put them in your data folder so we dont consider the\n",
        "# dot in 'mr.jackson' as the end of a sentence"
      ],
      "metadata": {
        "id": "L_vmRCdeDIUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 : Importing Dependencies"
      ],
      "metadata": {
        "id": "PKIcHrvtDWnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math \n",
        "import re\n",
        "import time # to see how long it takes in training\n"
      ],
      "metadata": {
        "id": "jYY54u1GDci7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers \n",
        "import tensorflow_datasets as tfds # tools for the tokenizer \n",
        "\n"
      ],
      "metadata": {
        "id": "KNAOyxcxDjA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 : Data Preprocessing "
      ],
      "metadata": {
        "id": "vxbhQ8ZfD1bF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "read files"
      ],
      "metadata": {
        "id": "lxZlpQSuU6xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data/europarl-v7.de-en.en\", mode='r', encoding=\"utf-8\") as f:\n",
        "    text_en = f.read()\n",
        "\n",
        "with open(\"data/europarl-v7.de-en.de\", mode='r', encoding=\"utf-8\") as f:\n",
        "    text_de = f.read()\n",
        "\n",
        "print(text_en[:50])\n",
        "print(text_de[:50])\n",
        "\n"
      ],
      "metadata": {
        "id": "FOaEgZaMU8oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data/nonbreaking-prefix.en\", mode='r', encoding=\"utf-8\") as f: \n",
        "    non_breaking_prefix_en = f.read()\n",
        "\n",
        "with open(\"data/nonbreaking-prefix.de\", mode='r', encoding=\"utf-8\") as f:\n",
        "    non_breaking_prefix_de = f.read()\n",
        "\n",
        "print(non_breaking_prefix_en[:5])\n",
        "print(non_breaking_prefix_de[:5])\n"
      ],
      "metadata": {
        "id": "af_Uyge_WB3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning"
      ],
      "metadata": {
        "id": "VPkHOBp7WtXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 해석 필요 \n",
        "for prefix in non_breaking_prefix_en:\n",
        "    text_en = text_en.replace(prefix, prefix + '###')\n",
        "\n",
        "text_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", text_en)\n",
        "text_en = re.sub(r\"\\.###\",'',text_en)\n",
        "text_en = re.sub(r\" +\", ' ', text_en)\n",
        "text_en = text_en.replace('###',' ')\n",
        "\n",
        "text_en = text_en.split(\"\\n\")\n",
        "\n",
        "for prefix in non_breaking_prefix_de:\n",
        "    text_de = text_de.replace(prefix, prefix + '###')\n",
        "text_de = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", text_de)\n",
        "text_de = re.sub(r\"\\.###\",'',text_de)\n",
        "text_de = re.sub(r\" +\",' ',text_de)\n",
        "text_de = text_de.replace('###',' ')\n",
        "\n",
        "text_de = text_de.split(\"\\n\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zoqLcKylWvEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing\n"
      ],
      "metadata": {
        "id": "5AI7L1MFgjYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    text_en, target_vocab_size=8000\n",
        ")\n",
        "\n",
        "tokenizer_de = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    text_de, target_vocag_size=8000\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y4i-lX14glIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_DE = tokenizer_de.vocab_size + 2 \n",
        "\n",
        "# we put start and tokens as size-1 and size-2 which are the same as \n",
        "# tokenizer_size and tokenizer_size +1 because the words are from [0 to ts -1]\n",
        "# tokenizer_en.encode(sentence) give a list then list + list + list appends them\n",
        "\n",
        "input = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "         for sentence in text_en]\n",
        "\n",
        "outputs = [[VOCAB_SIZE_DE-2] + tokenizer_de.encode(sentence) + [VOCAB_SIZE_DE-1]\n",
        "          for sentence in text_de]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vo_mI4FjiEFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove too long sentences\n",
        "\n",
        "- Why? (1) because when we pad we will have a hugeeee ram issuie for example sentence sizes of 1,100,2 when we pad they become 100,100,100 which we would rather loose that 100 than pad all to 100 (2) takes too much time to train"
      ],
      "metadata": {
        "id": "q0zRK-GSlFf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 20 # we will still have a lot of data with max len of 20 \n",
        "\n",
        "# this part. why we do it is a bit tricky. pay attention why we do it like this:\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "if len(sent) > MAX_LENGTH]\n",
        "\n",
        "# we remove in reversed because of shifting issuies when we satrt from begining\n",
        "for idx in reversed(idx_to_remive):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        "# same stuff for outputs > 20 \n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "if len(sent) > MAX_LENGTH]\n",
        "\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "kUgqSpjElLKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### input / output creation"
      ],
      "metadata": {
        "id": "XkXUBHnZnsWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. padding\n",
        "2. batching"
      ],
      "metadata": {
        "id": "HEcaUJNAnvL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen = MAX_LENGTH)\n",
        "\n",
        "outpus = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen = MAX_LENGTH)\n",
        "\n"
      ],
      "metadata": {
        "id": "qpQ7c_ULnzMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE =64\n",
        "BUFFER_SIZE = 20000 # how much data to keep\n",
        "\n",
        "# now we turned our data into a dataset \n",
        "dataset = tf.data.Dataset.from_tensort_slices((inputs, outputs))\n",
        "\n",
        "#this is something that improves the way the dataset is stored. it increases\n",
        "# the speed of accessing the data which increases training speed in return :\n",
        "data = dataset.cache()\n",
        "\n",
        "#now we shuffle in batches\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "#this increases the speed even further:\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "6q18koyeoso7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 : Model Building\n",
        "\n",
        "- A - Positional Encoding ( look at the formula in the paper)"
      ],
      "metadata": {
        "id": "CJqzmua2tLcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self):\n",
        "        # this positional encoder we made it a child of the Layers so it has all\n",
        "        # the properties that a layer has \n",
        "        super(PositionalEncodeing, self).__init__()\n",
        "\n",
        "    def get angles(self, pos, i, d_model):\n",
        "        \"\"\"\n",
        "        :pos: (seq_len, 1) index of the word in sentence [0 to 19]\n",
        "        :i: the dimensions of the embedding (glove dims 200) then-> [0 to 199]\n",
        "        :d_model: the size (dimension) of the embeded (e.g. glove size 200)\n",
        "        :return: (seq_len, d_model) why? we are getting the encoding of the\n",
        "                every positions vs every one of the dimensions of that word\n",
        "        \"\"\"\n",
        "        angles = 1 / np.power(10000., (2*(i//2))/np.float32(d_model))\n",
        "        return pos * angles # dim: (seq_len, d_model)"
      ],
      "metadata": {
        "id": "ixeXGTqMtZMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8jyRV040xmYk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}