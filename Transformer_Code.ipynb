{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer Code.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMZRYULlvU+cNtn/G9Q/Bly",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/21-study-paper-review/blob/main/Transformer_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer : Attention is All You Need"
      ],
      "metadata": {
        "id": "84pSvpZ_7DZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2021년 기준으로 최신 고성능 모델들은 Transformer 아키텍처를 기반으로 함 \n",
        "\n",
        "GPT : Transformer의 디코더 아키텍처를 활용\n",
        "BERT : Transformer의 인코더 아키텍처를 활용\n",
        "\n"
      ],
      "metadata": {
        "id": "_6sT2chJ7Ict"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "마운트 구글 드라이브"
      ],
      "metadata": {
        "id": "EV3iB6gOB_DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%pwd\n",
        "\n",
        "\"\"\" \n",
        "Use this javascript code in inspect>console so you wont need to click the page every 15 min:\n",
        "\n",
        "########################\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "########################\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0_3MDywzB0tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "change current path to where the working project folder is at "
      ],
      "metadata": {
        "id": "9ZJ_yddaCgwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/projects/transformers_translation/"
      ],
      "metadata": {
        "id": "7q2JK-gHCKG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0 : Get The Data "
      ],
      "metadata": {
        "id": "BNXzSWPXCrNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "upload the data to our current path and unzip it (numcomment and run this only once)"
      ],
      "metadata": {
        "id": "a9SU629ICwd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # data is from: https://www.statmt.org/europarl/ you can use this or just upload your own data\n",
        "# %cd data\n",
        "# !wget https://www.statmt.org/europarl/v7/de-en.tgz\n",
        "# !tar -xvf de-en.tgz\n",
        "# %cd ..\n",
        "# %pwd"
      ],
      "metadata": {
        "id": "Ab7v2fU2DAuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get non breaking prefixs"
      ],
      "metadata": {
        "id": "6-xucs7EDDW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get non_breaking_prefixes from https://github.com/moses-smt/mosesdecoder/tree/master/scripts/share/nonbreaking_prefixes\n",
        "# then rename them to: \"nonbreaking_prefix.en\" and \"nonbreaking_prefix.de\" and put them in your data folder so we dont consider the\n",
        "# dot in 'mr.jackson' as the end of a sentence"
      ],
      "metadata": {
        "id": "L_vmRCdeDIUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 : Importing Dependencies"
      ],
      "metadata": {
        "id": "PKIcHrvtDWnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math \n",
        "import re\n",
        "import time # to see how long it takes in training\n"
      ],
      "metadata": {
        "id": "jYY54u1GDci7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers \n",
        "import tensorflow_datasets as tfds # tools for the tokenizer \n",
        "\n"
      ],
      "metadata": {
        "id": "KNAOyxcxDjA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 : Data Preprocessing "
      ],
      "metadata": {
        "id": "vxbhQ8ZfD1bF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "read files"
      ],
      "metadata": {
        "id": "lxZlpQSuU6xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data/europarl-v7.de-en.en\", mode='r', encoding=\"utf-8\") as f:\n",
        "    text_en = f.read()\n",
        "\n",
        "with open(\"data/europarl-v7.de-en.de\", mode='r', encoding=\"utf-8\") as f:\n",
        "    text_de = f.read()\n",
        "\n",
        "print(text_en[:50])\n",
        "print(text_de[:50])\n",
        "\n"
      ],
      "metadata": {
        "id": "FOaEgZaMU8oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data/nonbreaking-prefix.en\", mode='r', encoding=\"utf-8\") as f: \n",
        "    non_breaking_prefix_en = f.read()\n",
        "\n",
        "with open(\"data/nonbreaking-prefix.de\", mode='r', encoding=\"utf-8\") as f:\n",
        "    non_breaking_prefix_de = f.read()\n",
        "\n",
        "print(non_breaking_prefix_en[:5])\n",
        "print(non_breaking_prefix_de[:5])\n"
      ],
      "metadata": {
        "id": "af_Uyge_WB3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning"
      ],
      "metadata": {
        "id": "VPkHOBp7WtXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 해석 필요 \n",
        "for prefix in non_breaking_prefix_en:\n",
        "    text_en = text_en.replace(prefix, prefix + '###')\n",
        "\n",
        "text_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", text_en)\n",
        "text_en = re.sub(r\"\\.###\",'',text_en)\n",
        "text_en = re.sub(r\" +\", ' ', text_en)\n",
        "text_en = text_en.replace('###',' ')\n",
        "\n",
        "text_en = text_en.split(\"\\n\")\n",
        "\n",
        "for prefix in non_breaking_prefix_de:\n",
        "    text_de = text_de.replace(prefix, prefix + '###')\n",
        "text_de = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", text_de)\n",
        "text_de = re.sub(r\"\\.###\",'',text_de)\n",
        "text_de = re.sub(r\" +\",' ',text_de)\n",
        "text_de = text_de.replace('###',' ')\n",
        "\n",
        "text_de = text_de.split(\"\\n\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zoqLcKylWvEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing\n"
      ],
      "metadata": {
        "id": "5AI7L1MFgjYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    text_en, target_vocab_size=8000\n",
        ")\n",
        "\n",
        "tokenizer_de = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    text_de, target_vocag_size=8000\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y4i-lX14glIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_DE = tokenizer_de.vocab_size + 2 \n",
        "\n",
        "# we put start and tokens as size-1 and size-2 which are the same as \n",
        "# tokenizer_size and tokenizer_size +1 because the words are from [0 to ts -1]\n",
        "# tokenizer_en.encode(sentence) give a list then list + list + list appends them\n",
        "\n",
        "input = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "         for sentence in text_en]\n",
        "\n",
        "outputs = [[VOCAB_SIZE_DE-2] + tokenizer_de.encode(sentence) + [VOCAB_SIZE_DE-1]\n",
        "          for sentence in text_de]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vo_mI4FjiEFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove too long sentences\n",
        "\n",
        "- Why? (1) because when we pad we will have a hugeeee ram issuie for example sentence sizes of 1,100,2 when we pad they become 100,100,100 which we would rather loose that 100 than pad all to 100 (2) takes too much time to train"
      ],
      "metadata": {
        "id": "q0zRK-GSlFf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 20 # we will still have a lot of data with max len of 20 \n",
        "\n",
        "# this part. why we do it is a bit tricky. pay attention why we do it like this:\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "if len(sent) > MAX_LENGTH]\n",
        "\n",
        "# we remove in reversed because of shifting issuies when we satrt from begining\n",
        "for idx in reversed(idx_to_remive):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        "# same stuff for outputs > 20 \n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "if len(sent) > MAX_LENGTH]\n",
        "\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "kUgqSpjElLKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### input / output creation"
      ],
      "metadata": {
        "id": "XkXUBHnZnsWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. padding\n",
        "2. batching"
      ],
      "metadata": {
        "id": "HEcaUJNAnvL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen = MAX_LENGTH)\n",
        "\n",
        "outpus = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen = MAX_LENGTH)\n",
        "\n"
      ],
      "metadata": {
        "id": "qpQ7c_ULnzMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE =64\n",
        "BUFFER_SIZE = 20000 # how much data to keep\n",
        "\n",
        "# now we turned our data into a dataset \n",
        "dataset = tf.data.Dataset.from_tensort_slices((inputs, outputs))\n",
        "\n",
        "#this is something that improves the way the dataset is stored. it increases\n",
        "# the speed of accessing the data which increases training speed in return :\n",
        "data = dataset.cache()\n",
        "\n",
        "#now we shuffle in batches\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "#this increases the speed even further:\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "6q18koyeoso7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 : Model Building\n",
        "\n",
        "- A - Positional Encoding ( look at the formula in the paper)"
      ],
      "metadata": {
        "id": "CJqzmua2tLcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "    \n",
        "    def __init__(self):\n",
        "        # 이 위치 인코더는 레이어의 하위 항목으로 만들어 레이어가 가지고 있는 모든 속성을 가집니다\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        \"\"\"\n",
        "        :pos: (seq_len, 1) 문장 내 단어의 색인 [0 ~ 19]\n",
        "        :i: 임베딩의 치수 (각각 200 치수) 그 다음> [0 ~ 199]\n",
        "        :d_model: 내장형 크기(예: glove  크기 200)\n",
        "        :return: (seq_len, d_model) 왜? 우리는 그 단어의 모든 위치 대 모든 차원의 인코딩을 얻고 있다.\n",
        "        \"\"\"\n",
        "           # PE (pos,2i) = sin (pos / 10000^(2i / d_model) )\n",
        "        # PE (pos,2i+1) = cos (pos / 10000^(2i / d_model) )\n",
        "        angles = 1 / np.power(10000., (2*(i//2))/np.float32(d_model))\n",
        "        return pos * angles # dim: (seq_len, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # input.shape = [batch_size, multihead_size(sz=8), each word (pos), that words embedding]\n",
        "        # 우리는 그들의 위치를 고려하여 입력값을 변경하지 않고, 우리는 단지 입력으로부터 딤을 얻고 완전히 개별적으로 pos 인코딩을 계산하여 끝에 쌓을 뿐이다.\n",
        "        seq_length = inputs.shape.as_list()[-2] # basically the pos\n",
        "        d_model = inputs.shape.as_list()[-1] # basically the embedded values\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        # 우리는 이것을 할 수 있다. 왜냐하면 입력과 인코딩이 같은 치수를 가져야 하기 때문이다. 그래서 우리는 0을 넣지 않는 새로운 축을 만들기 위해서… 모든 배치에 대해 같은 조광을 복사하고...\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        # 이제 우리는 입력과 그들의 pos_encodings를 둘 다 반환해야 하지만 우리는 np에 pos_codings가 있어서 그것들을 tf로 만든다\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)\n",
        "\n"
      ],
      "metadata": {
        "id": "ixeXGTqMtZMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B - Attention \n",
        " - Attention computation ( see the formula in the paper)"
      ],
      "metadata": {
        "id": "8jyRV040xmYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    # Q*K는 [output_len, d_model] * [d_model, input_len]이며 영어와 프랑스어 모두 20입니다.\n",
        "    # transpose_b = True는 키를 키 방향으로 돌리게 됩니다. T는 각각 이 dim : [batch_size, nb_proj, seq_len, d_proj]이므로 전치 시 [a,b,c,d] * []가 된다.\n",
        "    product = tf.matmul(queries, keys, transpose_b = True)\n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32) # makes the dim_num float\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim) # scalse it (formula stuff)\n",
        "\n",
        "    #왜냐하면 논문이 말한 이 마스크는 프로그램이 feauture를 보는 것을 막기 위해 선택적이기 때문이다. \n",
        "    #왜냐하면 우리가 역프로포즈를 할 때 그들은 그들 앞에 있는 것들을 고려할 것이기 때문에 \n",
        "    #우리는 이것을 멈추기 위해 그들에게 -1e9를 더해서 소프트맥스 후에 그들에게 0이 된다.\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_product +=(mask * -1e9)\n",
        "\n",
        "    #소프트맥스를 마지막 축을 따라 적용한다. 왜냐하면 우리는 소프트맥스의 합이 input_len에 1 scale_product = [output_len] -> softmax on input_len이 되기를 원하기 때문에 \n",
        "    #기본적으로 out_len에 대한 프로브를 동일하게 유지하지만 out_len에 대한 프로브를 찾는다.\n",
        "    probs = tf.nn.softmax(scaled_product, axis = -1)\n",
        "\n",
        "    # attention = [output_len. input_len] * [input_len, d_model] = [output_len, d_model]\n",
        "    # 그래서 이제 우리는 각각의 출력 단어에 대한 d_model 가중치를 가지고 있으며, 각각의 out_lens에 대한 예측을 보기 위해 앞으로 전달할 것이다.\n",
        "    attention = tf.matmul(probs, value)\n",
        "\n",
        "    return attention\n",
        "    "
      ],
      "metadata": {
        "id": "_ClRwS_i6iCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# 이것은 단지 당신이 dims에 대한 코드의 줄에서 어떤 일이 일어나는지 보기 위한 테스트이다 \n",
        "#(4dimes에서 우리가 깨달은 것은 matmul이 다른 dims를 배치 크기와 다른 것으로 간주하기 때문에\n",
        "# 마지막 두 개에 대해서만 멀티를 수행하였다). (tf.matmul(a, b, transpose_b= True)\n",
        "\n",
        "# a = np.arange(24).reshape(1,2,3,4)\n",
        "# a = tf.convert_to_tensor(a, np.float32)\n",
        "# b = np.arange(24).reshape(1,2,3,4)\n",
        "# b = tf.convert_to_tensor(b, np.float32)\n",
        "# product = tf.matmul(a, b, transpose_b=True)\n",
        "# print(product.shape)"
      ],
      "metadata": {
        "id": "UBtZ9LzlEa6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head attention sublayer"
      ],
      "metadata": {
        "id": "0yQqg3VGEovk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, nb_proj):\n",
        "        \"\"\"\n",
        "        :nb_proj : the number of projections for the multihead\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.np_proj = nb_proj\n",
        "\n",
        "    \n",
        "    #이것은 init와 동일하지만 우리가 객체를 처음으로 사용할 때 발생한다, init에서는 객체를 만들 때 호출되었다.\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        # 우리는 그것들이 분리될 수 있는지 확인하고 싶다.\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        # 우리는 그것을 정수로 만들기 위해 2개의 슬래시를 사용한다.\n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "\n",
        "        self.query_lin = layers.Dense(self.d_model)\n",
        "        self.key_lin = layers.Dense(self.d_model)\n",
        "        self.value_lin = layers.Dense(self.d_model)\n",
        "        self.final_lin = layers.Dense(self.d_model)\n",
        "\n",
        "    def split_proj(self, inputs, batch_size):\n",
        "        \"\"\"\n",
        "        : inputs : [batch_size, seq_len(20), d_model(prev layer dim)]\n",
        "\n",
        "        : return: \n",
        "            dims = [batch_size, nb_proj, seq_len, d_proj]\n",
        "            nb_proj는 cnn의 채널과 같습니다. 기본적으로 d_model을 nb_proj * d_proj로 분할하여 d_model/nb_proj를 찾습니다.\n",
        "        \"\"\"        \n",
        "        new_shape = (batch_size, -1, self.nb_proj, self.d_proj)\n",
        "        #here we will get: [ batch_sz, seq_len, nb_proj, d_proj]\n",
        "\n",
        "        splited_inputs = tf.reshape(inputs, shape=new_shape)\n",
        "\n",
        "        # so we need to reshape it to : [batch_size, nb_proj, seq_len, d_proj]\n",
        "        return tf.transpose(splited_inputs, perm=[0,2,1,3])\n",
        "\n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "\n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(vlaues)\n",
        "\n",
        "        # 이제 우리는 프로즈를 만들기 위해 그것들을 나누었다.\n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "\n",
        "        #each of the q, k , v are [batch_size, nb_proj, seq_len, d_proj]\n",
        "        attention = scaled_dot_product_attention(queries, keys, vlaues, mask)\n",
        "\n",
        "        # 이제 위에서 했던 갈라진 부분을 뒤집을 거야 : reshape + concat \n",
        "        attention = tf.greanspose(attention, perm=[0,2,1,3])\n",
        "        # we have [batch_size, seq_len, nb_proj, d_proj] so now we concat 2,3 \n",
        "        concat_attention = tf.reshapre(attention, shape=(batch_size, -1, self.d_model))\n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        return outputs \n"
      ],
      "metadata": {
        "id": "bZojqtrvEr_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C - Encoder "
      ],
      "metadata": {
        "id": "BVBnYTrhVbCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout):\n",
        "        \"\"\"\n",
        "        :FFN_units: \n",
        "            feed forward networks units : the number of units for the\n",
        "            feed forward which you can see in the encoder part of the \n",
        "            paper ( right after the attention there is a feed forward...)\n",
        "        : nb_project:\n",
        "            the number of projections we have (8)\n",
        "\n",
        "        : dropout : \n",
        "            the dropout rate e.g. 0.3 \n",
        "        \"\"\"\n",
        "\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout = dropout\n",
        "    \n",
        "    # 우리는 인코더를 만들 때 우리가 원하는 많은 바들이 없기 때문에 이것을 사용한다. 그래서 우리가 대신 '빌드' 함수를 사용할 때 우리는 그것들을 얻을 수 없다.\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        # we first build the object for the multi-head-attention\n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dense_1 = layers.Dense(units = self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units= self.d_model, activation=\"relu\")\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        \"\"\"\n",
        "        : mask: which we will apply in the multi-head attention\n",
        "        : training:\n",
        "            모델이 오버피팅되지 않도록 교육하는 동안 드롭아웃을 사용합니다=참\n",
        "            하지만 우리는 테스트만 할 때 그것을 사용하지 않습니다(일명 train=false).\n",
        "        \"\"\"\n",
        "        # 아키텍처를 살펴보면 인코더의 모든 쿼리/키/밸이 이전 계층에서 얻은 입력과 동일한 어레이임을 알 수 있습니다.\n",
        "        attention = self.multi_head_attention(inputs, inputs, inputs, mask)\n",
        "\n",
        "        # dropout + normalization after the attention\n",
        "        attention = self.dropout_1(attention, training = training)\n",
        "        # we do + inputs here 이유는 아키텍처에서 그것들이 결과 주의에 대한 이전 입력에 여전히 일치하기 때문에 우리는 그것을 정규화한다.\n",
        "        attention = self.norm_1(attention+inputs)\n",
        "\n",
        "        # now we do the dense in our FFN:\n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "\n",
        "        return outputs \n",
        "\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "4Mw3-CmzVcpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_encoding_layers,\n",
        "                 FFN_units, \n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        # we put name = name here because the name is something that belongs to the layers class. so we tell it to use name=\"encoder\"\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_encoding_layers = nb_encoding_layers # the number of encoders in a row \n",
        "        self.d_model = d_model # the size of the output e.g glove(200)\n",
        "\n",
        "        # we give vocab size for it to know the maxium number used in vocab \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units, nb_proj, dropout)\n",
        "            for _ in range(self.nb_encoding_layers)]\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        # look at the paper's architecture while doing these embedding with maybe glove eights .. \n",
        "        outputs = self.embedding(inputs)\n",
        "        # 우리가 이것을 한 이유는 3,4항의 종이에 쓰인 것 때문에 그들은 그들이 곱하기라고 말했다.\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        #this will give us the concat : outputs + pos_encoding\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        #이제 우리는 모든 인코딩 레이어 전에 드롭아웃을 한다.\n",
        "        # we give it training=bool -> so dont do dropout when training=false\n",
        "        outputs = self.dropout(outputs, training)\n",
        "\n",
        "        # 지금은 EmbeddingLayer를 한 번이 아니라 여러 번 해요.\n",
        "        for i in range(self.nb_encoding_layers):\n",
        "            # 따라서 다음 매개 변수를 사용하여 각 (i)번째 인코더에 적용합니다.\n",
        "            outputs = self.enc_layers[i](outpus, mask, training)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eHDQ7OowXecI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D - Decoder "
      ],
      "metadata": {
        "id": "BN3LZIaIlgSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    def __init__(self, FFN_units, nb_proj, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout = dropout \n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "\n",
        "        # MHA 1\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # MHA 2 \n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        #FFN\n",
        "        self.dense_1 = layers.Dense(units = self.FFN_units, activation='relu')\n",
        "        self.dense_2 = layers.Dense(units = self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate = self.dropout)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        # check the architecture in the paper to see why we do these\n",
        "\n",
        "        # this is the 1# attention \n",
        "        attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)\n",
        "        # we give it training=bool -> so dont do dropout when training=false \n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "\n",
        "        # this is the 2# attention. this is ALOT different than before one pay attention\n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        \n",
        "        #we give it training=bool -> so dont do dropout when training=fasle\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + inputs)\n",
        "\n",
        "        # the denses \n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "\n",
        "        return outputs \n",
        "\n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "riIFVwC9liRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_decoding_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.nb_decoding_layers = nb_decoding_layers # the number of encoders in a row \n",
        "        self.d_model = d_model # the size of the output e.g glove(200)\n",
        "\n",
        "        # we give vocab size for it to know the maximum number used in vocab \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(FFN_units, nb_proj, dropout)\n",
        "            for _ in range(nb_decoding_layers)]\n",
        "\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        # look at the paper's architecture while doing these embedding with maybe glove weights ... \n",
        "        outputs = self.embedding(inputs)\n",
        "        # the reason why we did this was vecause of what was writtent on the paper in secssion 3.4  which they said they multiplied\n",
        "        # it by squt of d_model \n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        #this will give us the concat : outputs + pos_encoding\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        #now we do dropout before all the encoding layers \n",
        "        # we give it training=bool -> so dont do dropout when training=false \n",
        "        outputs = self.dropout(outputs, training)\n",
        "\n",
        "        # now we do the EmbeddingLayer a couple of times, not just once\n",
        "        for i in range(self, nb_decoding_layers):\n",
        "            # so we apply it to the (i)th encoder in each for with these params:\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2, \n",
        "                                         training)\n",
        "        return outputs \n",
        "\n",
        "        \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_eMv8gVy0FV6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}