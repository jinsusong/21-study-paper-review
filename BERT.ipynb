{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM34bVq10vs6iCnpJW3r4hN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/21-study-paper-review/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "우리는 BERT라는 새로운 언어 표현 모델을 소개하는데, 이는 트랜스포머의 양방향 인코더 표현을 의미한다.\n",
        "\n",
        "최근의 언어 표현 모델(Peters 등, 2018a; Radford 등, 2018)과 달리, BERT는 모든 계층에서 왼쪽과 오른쪽 문맥을 공동으로 조절함으로써 레이블이 없는 텍스트로부터 깊은 양방향 표현을 사전 훈련하도록 설계되었다.\n",
        "\n",
        "결과적으로, 사전 훈련된 BERT 모델은 단 하나의 추가 출력 레이어로 미세 조정하여 실질적인 작업별 아키텍처 수정 없이 질문 답변 및 언어 추론과 같은 광범위한 작업에 대한 최신 모델을 생성할 수 있다.\n",
        "\n",
        "BERT는 개념적으로 단순하고 경험적으로 강력하다.\n",
        "\n",
        "GLUE 점수를 80.5%(7.7% 포인트 절대 개선), MultiNLI 정확도를 86.7%(4.6% 절대 개선), SQuAD v1.1 질문에 대답하는 SQuAD v1.2(1.5 포인트 절대 개선) 및 SQU를 포함하여 11개의 자연어 처리 작업에 대한 새로운 최첨단 결과를 얻었다.mprovision)"
      ],
      "metadata": {
        "id": "LGGZhl90UGOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "언어 모델 사전 훈련은 많은 자연어 처리 과제를 개선하는 데 효과적인 것으로 나타났다(Dai and Le, 2015; Peters 등, 2018a; Radford 등, 2018; Howard and Ruder, 2018).\n",
        "\n",
        "여기에는 자연어 추론(Bowman et al., 2015; Williams et al., 2018)과 패러프레이싱(Paraphrasing, Dolan and Brokett, 2005)과 같은 문장 수준 작업이 포함되며,\n",
        "\n",
        "명명된 엔티티 인식과 질문 답변과 같은 토큰 수준 작업도 포함된다.\n",
        "\n",
        "토큰 수준에서 미세한 출력을 생산해야 한다(Tjong Kim Sang and De Meulder, 2003; Rajpurkar 등, 2016).\n",
        "\n",
        "사전 훈련된 언어 표현을 다운스트림 작업에 적용하는 기존 전략에는 기능 기반 및 미세 조정이라는 두 가지가 있다.\n",
        "\n",
        "ELMo(Peters 등, 2018a)와 같은 기능 기반 접근법은 사전 훈련된 표현을 추가 기능으로 포함하는 작업별 아키텍처를 사용한다.\n",
        "\n",
        "생성 사전 훈련 트랜스포머(OpenAI GPT)와 같은 미세 조정 접근법은 최소한의 작업별 매개 변수를 도입하며, 단순히 사전 훈련된 모든 매개 변수를 미세 조정하여 다운스트림 작업에 대해 훈련된다.\n",
        "\n",
        "두 가지 접근 방식은 사전 훈련 중에 동일한 객관적 기능을 공유하며, 여기서는 일반적인 언어 표현을 학습하기 위해 단방향 언어 모델을 사용한다.\n",
        "\n",
        "우리는 현재의 기술이 특히 미세 조정 접근법에 대해 사전 훈련된 표현의 힘을 제한한다고 주장한다.\n",
        "\n",
        "표준어 모델이 단방향성이라는 점이 가장 큰 한계이며, 이는 사전 교육 중에 사용할 수 있는 아키텍처의 선택권을 제한한다.\n",
        "\n",
        "예를 들어, OpenAIGPT에서 저자들은 모든 토큰이 Transformer의 셀프 어텐션 레이어(Vaswani et al., 2017)에 있는 이전 토큰에만 집중할 수 있는 알레프트 토크 아키텍처를 사용한다.\n",
        "\n",
        "이러한 제한은 문장 수준 작업에 최적이 아니며, 양쪽 방향의 컨텍스트를 통합하는 것이 중요한 질문 답변과 같은 토큰 수준 작업에 미세 조정 기반 접근법을 적용할 때 매우 해로울 수 있다.\n",
        "\n",
        "본 논문에서는 트랜스포머의 BERT: 양방향 인코더 표현을 제안하여 미세 조정 기반 접근 방식을 개선한다.\n",
        "\n",
        "BERT는 Cloze 과제(Taylor, 1953년)에서 영감을 받은 \"마스크 언어 모델\"(MLM) 사전 훈련 목표를 사용하여 이전에 언급된 단방향성 제약을 완화한다.\n",
        "\n",
        "마스크된 언어 모델은 입력에서 토큰 중 일부를 임의로 마스킹하며, 목표는 마스크된 단어의 문맥만을 기반으로 원래 어휘 ID를 예측하는 것이다.\n",
        "\n",
        "좌회전 언어 모델 사전 훈련과 달리, MLM 목표는 표현이 왼쪽과 오른쪽 맥락을 융합할 수 있게 해주므로 심층 양방향 트랜스포머를 사전 학습할 수 있다.\n",
        "\n",
        "마스킹된 언어 모델 외에도 텍스트 쌍 표현을 공동으로 전제하는 \"다음 문장 예측\" 작업도 사용한다. 본 논문의 기여는 다음과 같습니다.\n",
        "\n",
        "• 언어 표현을 위한 양방향 사전 훈련의 중요성을 설명한다.\n",
        "\n",
        "사전 훈련에 단방향 언어 모델을 사용하는 Radford 등(2018)과 달리, BERT는 마스크된 언어 모델을 사용하여 사전 훈련된 심층 양방향 표현을 가능하게 한다.\n",
        "\n",
        "이는 독립적으로 훈련된 왼쪽에서 오른쪽으로, 오른쪽에서 왼쪽으로의 LM의 얕은 연결을 사용하는 피터스 외 연구진(2018a)과도 대조된다.\n",
        "\n",
        "• 사전 훈련된 표현이 많은 중공학적 작업별 아키텍처의 필요성을 줄인다는 Weshow. BERT는 많은 작업별 아키텍처를 능가하는 대규모 문장 수준 및 토큰 수준 작업에서 최첨단 성능을 달성하는 최초의 미세 조정 기반 표현 모델이다.\n",
        "\n",
        "• BERT는 11개의 NLP 작업에 대한 최신 기술을 발전시킨다. 코드와 사전 훈련된 모델은 https://github.com/ google-discop/bert에서 이용할 수 있다."
      ],
      "metadata": {
        "id": "gvc5mp9kUC_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2 관련 작업\n",
        "일반 언어 표현을 사전 교육한 오랜 역사가 있으며, 이 섹션에서 가장 널리 사용되는 접근 방식을 간략히 검토한다.\n",
        "\n",
        "2.1 비지도 특징 기반 접근법 \n",
        "광범위한 적용 가능한 단어 표현 학습은 수십 년 동안 비신경적(Brown 등, 1992년; 안도 및 장, 2005년; Blitzer 등, 2006년) 및 신경적(Mikolov 등, 2013년; Pennington 등, 2014년) 방법을 포함한 활발한 연구 영역이었다. \n",
        "\n",
        "사전 훈련된 단어 임베딩은 현대 NLP 시스템의 필수적인 부분으로, 처음부터 학습한 임베딩에 비해 상당한 개선을 제공한다\n",
        "(Turian et al., 단어 내장 벡터를 사전 훈련하기 위해 왼쪽에서 오른쪽으로 언어 모델링 목표(Mnih 및 힌튼, 2009)와 \n",
        " 왼쪽과 오른쪽 문맥에서 정확한 단어와 부정확한 단어를 구별하는 목표(Mikolov 등, 2013)가 사용되었다.\n",
        "\n",
        "이러한 접근법은 문장 임베딩(Kiros 등, 2015; Logeswaran 및 Lee, 2018) 또는 문단 임베딩(Le and Mikolov, 2014)과 같이 보다 세분화된 것으로 일반화되었다. \n",
        " \n",
        " 문장 표현을 훈련하기 위해, 이전 연구에서는 다음 문장(Jernite et al., 2017; Logeswaran and Lee, 2018), \n",
        " \n",
        " 이전 문장의 표현이 주어진 다음 문장 단어의 왼쪽에서 오른쪽으로 생성(Kiros et al., 2015) 또는 자동 인코더 파생 목표의 삭제를 위해 목표를 사용했다(Hill et al., 2016). \n",
        " \n",
        " ELMo와 그 전신(Peters et al., 2017, 2018a)은 전통적인 단어 임베딩 연구를 다른 차원으로 일반화한다. \n",
        " \n",
        " 왼쪽에서 오른쪽으로, 오른쪽에서 왼쪽으로 언어 모델에서 상황에 맞는 특징을 추출한다. 각 토큰의 문맥적 표현은 왼쪽에서 오른쪽으로, 오른쪽에서 왼쪽으로의 표현이다. \n",
        " \n",
        " 상황별 단어 임베딩을 기존 과제별 아키텍처와 통합할 때, ELMo는 질문 답변(Rajpurkar 등, 2016), 감정 분석(Socher 등, 2013), 명명된 개체 인식(Tjong Kim Sang and Deulder, 2003)을 포함한 \n",
        " \n",
        " 몇 가지 주요 NLP 벤치마크(Peters 등, 2018a)에 대한 최신 기술을 발전시킨다. \n",
        " \n",
        " Melamud 등(2016)은 LSTM을 사용하여 왼쪽과 오른쪽 문맥 모두에서 단일 단어를 예측하는 작업을 통해 상황별 표현을 학습할 것을 제안했다. \n",
        " \n",
        " ELMo와 유사하게, 그들의 모델은 특징 기반이고 깊이 양방향적이지 않다. Fedus 등(2018)은 클로즈 작업을 사용하여 텍스트 생성 모델의 견고성을 향상시킬 수 있음을 보여준다.\n",
        "\n",
        "\n",
        " 2.2 감독되지 않은 미세 조정 접근법\n",
        "특징 기반 접근법과 마찬가지로, 이 방향으로 첫 번째 방법은 레이블이 없는 텍스트의 사전 훈련된 단어 임베딩 매개 변수만 작동한다(Colobert and Weston, 2008). \n",
        " \n",
        "보다 최근에는 상황별 토큰 표현을 생성하는 문장 또는 문서 인코더가 레이블이 없는 텍스트에서 사전 훈련되었고 감독된 다운스트림 작업에 맞게 조정되었다(Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., \n",
        "\n",
        "이러한 접근법의 장점은 처음부터 배울 필요가 있는 매개 변수가 거의 없다는 것이다. \n",
        "\n",
        "적어도 이러한 이점 때문에, 오픈AI GPT(Radford 등, 2018)는 GLUE 벤치마크(Wang 등, 2018a)의 많은 문장 수준 작업에서 이전의 최첨단 결과를 달성했다. \n",
        "\n",
        "왼쪽에서 오른쪽으로 언어 모델링과 자동 인코더 목표는 그러한 모델의 사전 교육에 사용되었다(Howard and Ruder, 2018; Radford 등, 2018; Dai and Le, 2015).\n",
        "\n",
        "2.3 지도 데이터로부터의 학습 전달\n",
        "또한 자연어 추론(Conneau 등, 2017)과 기계 번역(McCann 등, 2017)과 같이 대규모 데이터 세트를 가진 감독된 작업으로부터 효과적인 전송을 보여주는 작업도 있었다. \n",
        "\n",
        "컴퓨터 비전 연구는 또한 ImageNet으로 사전 훈련된 모델을 미세 조정하는 것이 효과적인 레시피인 사전 훈련된 대규모 모델에서 전이 학습의 중요성을 입증했다(Deng 등, 2009년; Yosinski 등, 2014년).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uVyZ-wceQsRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3 버트\n",
        "우리는 이 절에서 BERT와 그것의 자세한 구현을 소개한다. \n",
        "\n",
        "우리의 프레임워크에는 사전 훈련과 미세 조정의 두 단계가 있다. \n",
        "\n",
        "사전 훈련 동안 모델은 서로 다른 사전 훈련 작업에 대해 레이블이 없는 데이터에 대해 훈련된다. \n",
        "\n",
        "미세 조정을 위해 BERT 모델은 먼저 사전 훈련된 매개 변수로 초기화되며, 모든 매개 변수는 다운스트림 작업의 레이블링된 데이터를 사용하여 미세 조정된다. \n",
        "\n",
        "각 다운스트림 작업에는 사전 훈련된 동일한 매개 변수로 초기화되더라도 별도의 미세 조정된 모델이 있다. \n",
        "\n",
        "그림 1의 질문 답변 예제는 이 절의 실행 예제가 될 것입니다. \n",
        "\n",
        "BERT의 또 다른 특징은 여러 작업에 걸쳐 통합된 아키텍처이다. \n",
        "\n",
        "사전 훈련된 아키텍처와 최종 다운스트림 아키텍처 사이에는 최소한의 차이가 있습니다. \n",
        "\n",
        "모델 아키텍처 BERT의 모델 아키텍처는 Vaswani 등(2017)에 설명된 원래 구현에 기반한 다층 양방향 트랜스포머 인코더이며 텐서2tensor 라이브러리에 릴리스되었다.\n",
        "\n",
        "1 Transformer의 사용이 보편화되었고 구현이 원본과 거의 동일하기 때문에 모델 아키텍처에 대한 철저한 배경 설명을 생략하고 Vaswani 등(2017)과 \"주석이 달린 Transformer\"와 같은 우수한 가이드를 참조할 것이다.\n",
        "\n",
        "2 본 연구에서 우리는 계층 수(즉, Tr.)를 나타낸다.ansformer blocks)는 L로, 숨겨진 크기는 H로, 자기 주의 헤드의 수는 A.\n",
        "\n",
        "3으로 주로 BERTBASE(L=12, H=768, A=12, Total Parameters=110M) 및 BERTLARGE(L=24, H=16, Total Parametymeter)의 두 가지 모형 크기에 대한 결과를 보고합니다.비교를 위한 AI GPT. \n",
        "\n",
        "그러나 중요한 것은 BERT Transformer는 양방향 자기 주의를 사용하는 반면 GPT Transformer는 모든 토큰이 왼쪽 컨텍스트에만 집중할 수 있는 제한된 자기 주의를 사용합니다.\n",
        "\n",
        "4 1//github.com/tensorflow/tensor2tensor 2//nlp.seas.harvard.edu/2018/04/03/attention.html 모든 경우에 피드 포워드/필터 크기를 4H, 즉 H = 768의 경우 3072이고 H = 1024의 경우 4096입니다. \n",
        "\n",
        "4문헌에서 양방향 TransInput/Output 표현 BERT가 다양한 다운스트림 작업을 처리하도록 하기 위해 입력 표현은 단일 문장과 문장 쌍(예: 질문, 답변)을 모두 명확하게 나타낼 수 있다. 이\n",
        "\n",
        " 작업을 통해, \"문장\"은 실제 언어적 문장이라기보다는 임의의 범위의 연속된 텍스트일 수 있다. \n",
        " \n",
        " \"시퀀스\"는 BERT에 대한 입력 토큰 시퀀스를 가리키며, 한 문장이거나 두 문장이 함께 포장된 것일 수 있다. \n",
        " \n",
        " 우리는 30,000개의 토큰 어휘를 가진 워드피스 임베딩(Wu 등, 2016)을 사용한다. \n",
        " \n",
        " 모든 시퀀스의 첫 번째 토큰은 항상 특수 분류 토큰([CLS])이다. \n",
        " \n",
        " 이 토큰에 해당하는 마지막 숨겨진 상태는 분류 작업의 집계 시퀀스 표현으로 사용됩니다. 문장 쌍은 하나의 시퀀스로 함께 채워집니다. 우리는 두 가지 방법으로 문장을 구분합니다. \n",
        " \n",
        " 먼저 특수 토큰([SEP])으로 구분합니다. \n",
        " \n",
        " 둘째, 우리는 A 문장에 속하는지 B 문장에 속하는지 나타내는 학습된 임베딩을 모든 토큰에 추가한다. \n",
        " \n",
        " 그림 1에서와 같이, 우리는 입력 임베딩을 E로, 특수 [CLS] 토큰의 최종 은닉 벡터를 C rh RH로, ith 입력 토큰에 대한 최종 은닉 벡터를 Ti rh RH로 나타낸다. \n",
        " \n",
        " 주어진 토큰의 경우, 입력 표현은 해당하는 토큰, 세그먼트 및 위치 임베딩의 합계를 통해 구성된다. 이 구조의 시각화는 그림 2에서 볼 수 있다."
      ],
      "metadata": {
        "id": "PR2CoZK0T_t8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}